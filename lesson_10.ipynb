{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional and recurrent neural networks\n",
    "\n",
    "In the last lecture we introduced deep learning, talked about the need to go deep, and trained a deep neural network using `sklearn` and then using `tensorflow`. The networks that we trained then were examples of **dense** or **fully connected** networks, meaning that every feature in the previous layer is connected to every feature in the current layer. Because of this property, dense neworks are very likely to overfit and have a lot of parameters. Too many parameters means that we run into computational limits as we try to go deeper. \n",
    "\n",
    "In this lecture, we want to expand on the idea of deep learning by proposing some modifications to dense networks. The first modification will introduce the idea of **convolutional neuron networks** which learn features in such a way that we take **locality** into account. The next modification will introduce **recurrent neural networks** which learn features when we have sequential data (i.e. data where order matters). Both ideas rely on **shared parameters** to greatly reduce the number of parameters in a neural networks, allowing us to **go deeper**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST and convolutional networks\n",
    "\n",
    "Let's begin by loading the MNIST data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_train_onehot = to_categorical(y_train, num_classes = 10)\n",
    "# y_test_onehot = keras.utils.to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function will later be used to plot the training and validation data accuracy and loss for the models we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_test_performance(model):\n",
    "\n",
    "    fig = plt.figure(figsize = [10, 10])\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(model.history.history['accuracy'])\n",
    "    plt.plot(model.history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(model.history.history['loss'])\n",
    "    plt.plot(model.history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "    plt.tight_layout()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D convolutional neural networks we work with here expect the input data to be images. However, images are usually not 2D objects but 3D objects: the first two dimensions encode the size of the image, and the third dimension is for the **channel**. For example, a colored image oncoded as **RGB** (red, green, blue) has three channels. A greyscale image like the ones in the MNIST dataset have only one channel, but we still need the data to be 3D. Of course, the dataset itself then needs another dimension for the training example. This means our data has the following dimensionality:\n",
    "\n",
    "$$\\text{training examples} \\times \\text{row pixels} \\times \\text{column pixels} \\times \\text{channel}$$\n",
    "\n",
    "We need to make sure that our data abides by the dimensionality above, and we use `reshape` for that. We also normalize the data as we did in the previous lecture by dividing the pixel values by 255 so the pixel values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape (before):', X_train.shape)\n",
    "print('x_test shape (before):', X_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print('x_train shape (after):', X_train.shape)\n",
    "print('x_test shape (after):', X_test.shape)\n",
    "\n",
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, before training a new model, let's first train a baseline model so we can compare performances. As our baseline, we choose a neural network with only a single hidden layer with 128 neurons in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compile our model. To speed up the training, we use the Adam optimizer, which is more efficient than vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(0.01),\n",
    "    metrics = ['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our model, specifying a mini-batch size of 128 and a total of 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size = 128,\n",
    "    epochs = 20,\n",
    "    verbose = 1,\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we now plot accuracy and loss for the training and validation data, epoch by epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_train_test_performance(model_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train a convolutional neural network. Foretunately, there's very little we need to change in the code: The only changes show up in the architecture of the model, where we now use `Conv2D` and `MaxPool2D` layers initially. Note that the flat layers don't disappear. Instead they get pushed to the end of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_cnn = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)),\n",
    "  tf.keras.layers.MaxPool2D((2, 2)),\n",
    "  # tf.keras.layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n",
    "  # tf.keras.layers.MaxPool2D((2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "  tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(0.01),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "model_cnn.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size = 128,\n",
    "    epochs = 20,\n",
    "    verbose = 1,\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_performance(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Train the model for another two epochs by copying and pasting the **training portion** of the code above in the cell below. What is the starting accuracy at epoch 1? What does this tell you about the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a model with `keras`, assuming your model is called `model`, you can use `model.summary()` to print some useful information about the model. You can print the model summary prior to training, in order to have an idea of the complexity of the model.\n",
    "\n",
    "- Print the model summary for both the dense neural network and the CNN we trained above. How many parameters do each have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to save the trained model we can use the `save_weights` method. This is useful in case we (1) need to later train the model further, (2) fine-tune the model to a more specific task (called transfer learning), or (3) load the model in a scoring environment to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_digit.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMBD and recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now shift focus to recurrent neural networks. Recurrent networks become sort of demystified once we realize that they are just a regular neural network unrolled. Using `keras` we don't have to manually do the unrolling: the details are handled under the hood.\n",
    "\n",
    "For the next example, we use the IMDB dataset which consists of 25000 **movie reviews** for training and 25000 for testing. The label is the user's **sentiment** about the movie: positive or negative. We use recurrent neural networks to train a binary classifier to do sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbd = tf.keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we will split it into trining and testing. Note that we can decide the size of the corpus to use. This can be handy when we need to try our code and want to keep computations under control. By specifying for example to use a corpus with 10,000 words, we limit the reviews to use the most common 10,000 words. As we will later see, any word in the review that doesn't make the cut is replaced by a generic `<UNK>` marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imbd.load_data(num_words = num_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check the first row of the data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look much like a movie review, does it? The reason is that the data that keras is presenting us with has already been **pre-processed**, meaning that instead of looking at the raw movie review, each review has been turned into a numeric representation. One kind of representation we are already familiar with is one-hot-encoding. We can apply one-hot-encoding to this data too, and it would look like this:\n",
    "\n",
    "- get a list of all the unique words across **all** the reviews (we call this a **corpus**) and create dummy columns for each word\n",
    "- for each review let the word dummy column be 1 if the word is present in the review and 0 otherwise\n",
    "\n",
    "The problem with the above approach is that we end up with lots of dummy columns (one per word) and for any given review the majority of the encodings would be zero. One-hot encoding can still be a good idea, but we need to find a way to representing in a **sparse** format instead.\n",
    "\n",
    "One approach is to sort the corpus by most frequent to least frequent. We then index the words using this order, starting with index 1, so that the index 1 would encodes most frequent word in the corpus, 2 would be the second most frequent word, etc. We can download the **word to index dictionary** used by the IMBD data set to see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = tf.keras.datasets.imdb.get_word_index()\n",
    "print(\"There are {:,} in the corpus and indexing starts at {}.\".format(\n",
    "    len(word_to_index.keys()), min(word_to_index.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the word and index for the first 10 words. Note that `word_index` is a dictionary, so it is not necessarily sorted in the order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_10_keys = list(word_to_index.keys())[:10]\n",
    "\n",
    "for k in first_10_keys:\n",
    "    print(f\"For word \\\"{k}\\\" \\t the index is {word_to_index[k]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take this word to index and add a few more \"words\" to it. These are not really words, but markers that are used by the algorithm:\n",
    "\n",
    "By convention index 0 though 3 are reserved for the some special markers shown below. What these markers are will become clear later. We make room for these markers in the word to index dictionary by pushing the current indexes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {k:(v + 3) for k, v in word_to_index.items()}\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<START>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "word_to_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a **reverse dictionary**, that is one where we switch keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {value: key for key, value in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"For index {i} the word is \\\"{index_to_word[i]}\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's look at the first review in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function named `decode_review` that takes as input a review like the above one and uses `index_to_word` to return the actual review as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now decode the first review in the data using your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think the presence of `<UNK>` in the review means?\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we need to force all images in a convolutional network to have a same size by choosing a size and then padding smaller images around the edges until they have the same size. We need to do the same with our reviews: we need to pick a maximum number of words a review can have, and (1) trim longer reviews and (2) pad shorter reviews. Foretunately, `keras` already has a function for doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_review_length = 300\n",
    "x_train_padded = pad_sequences(x_train, maxlen = max_review_length)\n",
    "x_test_padded = pad_sequences(x_test, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first review looks like once it's padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our model. Instead of the vanilla recurrent neural network, we use a fancier kind called an LSTM. An LSTM is a recurrent network that is better at retaining long-term information, which is very important when dealing with language data. A vanilla RNN on the other hand tends to put much more emphasis on recent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 250\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length = max_review_length))\n",
    "# model.add(keras.layers.LSTM(32, return_sequences = True))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train our model. This is a model lots of parameters and can take a while to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_padded, y_train, validation_data = (x_test_padded, y_test), epochs = 2, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, lets look at the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test_padded, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- What does the model predict for the first review in the test data? HINT: you may need to use `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the first review in the test data using the function you wrote earlier to decode reviews. Does the prediction seem good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print 5 examples in the test data where the prediction is correctly classified, and 5 examples where the prediction does not match the label, i.e. where the model mis-classified. Read the reviews and see if the mis-classified reviews seem less clear-cut than the correctly classified reviews. To help you with this, we show you in the following cell how you can test whether the first review in the test data was correctly classified or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test_padded[0].reshape(1, -1))[0][0]\n",
    "obsv = y_test[0]\n",
    "if ((pred > 0.5) & (obsv > 0.5)) | ((pred < 0.5) & (obsv < 0.5)):\n",
    "    print(\"Correctly classfied.\")\n",
    "else:\n",
    "    print(\"Incorrectly classified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the impressive things about neural networks is how changes in their architectures can suddenly open them up to solving a new set of problems. Convolutional neural networks and recurrent neural networks are just two examples. In many ways the journey is only beginning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
