{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection refers to algorithms that can help us narrow down our **feature set** to a smaller set of important features, where importance here refers to the strength of the relationship between the feature and the target. A more important feature is one that ultimately can influence the model's prediction more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter-based methods for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter-based selection** refer to methods we can use to identify features with a high association and remove one as part of feature selection prior to machine learning. One way we can measure the degree to which two features are associated is using **correlation**, but correlation (**pearson's correlation coefficient**, to be specific) measures the strength of **linear association** meaning that when correlation is high between two features (close to 1 or -1) then their scatter plot looks like a straight line. If instead of a straight line the scatter plot followed a curved line, we need to use other measures such as **ranked correlation** or **mutual information**.\n",
    "\n",
    "In the following example, we generate two features $X$ and $Y$ which are related to each other based on a polynomial (curve-like) equation with some added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.linspace(start = -1, stop = 3, num = 401, endpoint = True)\n",
    "Y = X**2 - 2*X + 1 # Y is a polynomial function of X\n",
    "print(\"Standard Deviation of Y = {:0.2f}\".format(np.std(Y)))\n",
    "\n",
    "noise_var = 0.2\n",
    "noise = np.random.normal(0, noise_var, len(X))\n",
    "Y += noise # add noise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can can just plot the features against each other and see how they're related, but imagine having to do this for every pair of some 20 features. So instead we want to rely on some metric to do this, so that we can later simply filter out redundant features based on the metric. Let's see what the correlation coefficient is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X, Y)[0, 1]\n",
    "print(\"Correlation between X and Y is {:0.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the correlation is close to zero, and if we had not plotted $X$ against $Y$, we would have been led to believe that there is no association between them, but let's now use mutual information to capture the stregth of their association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency = c_xy)\n",
    "    return mi\n",
    "\n",
    "print(\"Mutual information = {:0.2f}\".format(calc_MI(X, Y, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Return to the above example and increase the noise gradually (using `noise_var`) and see how it changes correlation and mutual information.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper methods for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **embedded methods** for feature selection include **step-wise feature selection**, which is an iterative algorithm. It consists of this:\n",
    "\n",
    " - train a model with a set of features and evaluating its performance (accuracy)\n",
    " - see how much accuracy improves if we train a model but this time include a new and promising feature (this is called **forward selection**)\n",
    " - see how much accuracy goes down if we train a model but this time exclude an existing and not-so-useful feature (this is called **backward selection**)\n",
    " - repeat until model accuracy more-or-less peaks, leaving us with a **final model** whose selected features are a **subset** of all the features in the data (which is why step-wise selection is also called **best-subset selection**)\n",
    "\n",
    "In other words, every step in stepwise regression is one where we either include a promising new feature that was left out so far, or exclude a feature that was part of the model so far but doesn't seem to contribute much to model performance. In this sense, step-wise selection, just like filter-based selection attemps to include new features that are bringing in **new information** and avoid features that are either just noise or redundant. The difference is that step-wise selection does this in an iterative and automated fashion. The downside is that step-wise regression can be slow, since every step requires that we train a new model so we can evaluate its performance.\n",
    "\n",
    "Let's see how this works in action. We begin by generating a dataset using the `make_friedman1` function in `sklearn`. This function will intentially return a data with correlated features so that we can use it to do step-wise selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "X, y = make_friedman1(n_samples = 50, n_features = 10, random_state = 0)\n",
    "X = pd.DataFrame(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We can take a look at the correlation matrix to see which features are strongly correlated, but with 10 feature it's a lot of numbers to be sifting through. So one quick an easy way to do this is to create a heatmap from the correlation matrix. Find out how you can use the `seaborn` library to create a heatmap from the correlation matrix for the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to try our hand at step-wise feature selection. First we choose the algorithm we want to use for training, which is `LinearRegression` in our case. Notice that in the code below we refer to the algorithm as an `estimator`. This is a common term used in machine learning. We then pass `estimator` to a step-wise selection algorithm called `RFE`, which stands for **recursive feature elimination** (another way of saying **backward selection**). So `RFE` acts as a **wrapper** for `LinearRegression`. To run it, we just call `fit` and pass it the data (just like we would have done with `estimator` itself), except that now `fit` runs the iterative backward selection algorithm we described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE # Recursive Feature Elimination\n",
    "\n",
    "estimator = LinearRegression()\n",
    "back_selector = RFE(estimator, 5, step = 1, verbose = True) # select 5 features, removing 1 feature at a time\n",
    "back_selector = back_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the backward selection run started with 10 features, but ended with fewer, throwing out one feature at every step. We can see which features made the cut by running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mask of selected features:\")\n",
    "print(back_selector.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the order in which features were removed in the next cell, starting with the higher number. The features with rank 1 were the ones who made the cut (remained in the final model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ranking is the inverse of order of removal (higher rank is the first feature removed):\")\n",
    "print(back_selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we ran `fit`, we can now run `predict` to get the final model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_backward = back_selector.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, let's train the `estimator` (the original `LinearRegression` algorithm) on the whole data, without using backward selection and get predictions for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimator.fit(X, y)\n",
    "yhat_estimator = estimator.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's look at the correlation between (1) the labels and (2) the predictions when we do backward selection, and (3) the predictions when we don't do backward selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef([yhat_backward, yhat_estimator, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not be too surprised to see that the correlations are high, especially between the two predictions. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "There is one last sanity-check we can perform.\n",
    "\n",
    "First create a subset of the data with only those features that made the cut in backward selection. HINT: You can use `back_selector.support_` to filter the features that `back_selector` ended up with. Name the data `X_subset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize a new estimator with `LinearRegression` and call it `estimator_subset`. Then train `estimator_subset`, on the `X_subset` and obtain predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get the correlation matrix for (1) the label, (2) the predictions obtained by `estimator_subset` and (3) the predictions obtained by `back_selector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef([yhat_backward, yhat_estimator, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see a correlation of 1 between the two predictions, meaning the predictions are exactly the same. This is because backward selection returns a model trained on the subset of features that it deems the best, so manually training the model on that same subset should result in the same trained model.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of backward selection, we now try **forward selection**. Backward selection starts with all features and end with a subset, **dropping** a feature at each step. Forward selection starts with one feature and ends with a subset, **adding** a feature at each step. \n",
    "\n",
    "We can also do **step-wise selection** (using `SequentialFeatureSelector` in the `mlxtend` library), which starting with a subset of features, does **forward or backward** selection at each step, meaning that at each step it may choose to add a feature or drop a feature depending on which works better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "stepwise_selector = SFS(estimator, \n",
    "    k_features = \"parsimonious\", \n",
    "    forward = True, \n",
    "    floating = False, \n",
    "    scoring = 'neg_mean_squared_error', \n",
    "    cv = 10)\n",
    "\n",
    "stepwise_selector = stepwise_selector.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mlxtend` library gives us a plot for visualizing model performance as feature selection happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "fig = plot_sfs(stepwise_selector.get_metric_dict(), kind = 'std_err')\n",
    "plt.title('Sequential selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "So far we've been using an artificial dataset. Let's now switch to using a real dataset. We use the Boston housing dataset, which has information about houses in Boston and their sale price as a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "X = pd.DataFrame(X, columns = boston.feature_names)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data by reporting the number of rows, columns and plotting the scatter plot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did previously, use `SFS` to train a step-wise selection model using `LinearRegression` as the estimator. Call your model `stepwise_selector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model performance vs number of features. How many features would you recommend we use for predicting house prices in the Boston housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best subset of feature at each step, we use `stepwise_selector.get_metric_dict()` which has information about accuracy and a list of the final features **for every step** of step-wise selection. So for example, if we want to know what results look like at step 4, we run `stepwise_selector.get_metric_dict()[4]` (this object is indexed starting at 1). Report accuracy and the the names of the features that at step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded methods for feature selection\n",
    "\n",
    "The last thing we cover today are embedded methods for selecting features, which refer to using **regularization** so the algorithm can do feature selection by itself. No wrapping or iterating is required, although if we with to tune the **regularization constant** (also called **shrinkage**) then we have to train many times. The regularization constant is an example of a **hyper-parameter** and **hyper-parameter tuning** is the subject of another lecture. In the below example, the regularization constant is called `alpha` and we show you an example of how tuning `alpha` affects the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two examples of regularizaiton: LASSO and Ridge regression. Both LASSO and Ridge regression are implementations of linear regression where we try to minimize prediction error plus some penalty that depends on the model's parameters (or coefficients) and the shrinkage constant (`alpha` in the code below). LASSO penalizes the model's parameters using the sum of the **absolute values** of the parameters (this is also called **L1-regularization**), while Ridge does so based on the sum of the **squared values** of the parameters (this is also called **L2-regularization**). For reasons we cannot elaborate on here, **LASSO has the by-product that it also does feature selection**, whereas Ridge doesn't. So not all regularization results in feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "alpha = 0.25 # increasing alpha can shrink more variable coefficients down to 0\n",
    "clf = linear_model.Lasso(alpha = alpha)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# print(clf.intercept_)\n",
    "# print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the coefficients of the model we trained using a barplot. Notice how some of the coefficients are zero, meaning they were effectively dropped out of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame({'col': X.columns, 'coef': clf.coef_})\n",
    "ax = sns.barplot(x = 'col', y = 'coef', data = dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw the result of one model using `alpha = 0.25`. Now let's see what happens when we train many models with different values for `alpha`. To keep track of it all, we create a `DataFrame` where we store the choice of `alpha` we used and the model's coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['alpha', 'col', 'coef'])\n",
    "\n",
    "for alpha in np.arange(0.01, 5, .01):\n",
    "    clf = linear_model.Lasso(alpha = alpha)\n",
    "    clf.fit(X, y)\n",
    "    dd = pd.DataFrame({'col': X.columns, 'coef': clf.coef_})\n",
    "    dd['alpha'] = alpha\n",
    "    results = results.append(dd, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the value of coefficients against `alpha` to see what happens to the coefficents as we increase the shrinkage constant `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x = 'alpha', y = 'coef', hue = 'col', data = results)\n",
    "ax.set(ylim = (-2.5, 2.5))\n",
    "ax.legend(loc = 'center right', bbox_to_anchor = (1.5, 0.5), ncol = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, as `alpha` goes up, the coefficients get smaller and smaller. More importantly,  some coefficients like `NOX` quickly converge to zero (get dropped out of the model), while others like `RM` shrink slowly and slowly. So by increasing `alpha`, we're able to shrink the less important features to zero and end up with fewer features. This is why we say that LASSO regression has the **by-product** of also performing **feature selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a `LinearRegression` model so we can compare the regularized model (LASSO) to the non-reguralized model (`LinearRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X, y)\n",
    "pred_lm = lm.predict(X) # predictions when using LinearRegression\n",
    "pred_clf = clf.predict(X) # prediction when using LASSO\n",
    "\n",
    "np.corrcoef(pred_lm, pred_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the high correlation indicates that our predictions are very close. So dropping a few feature from the model has very little effect on the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "from sklearn import linear_model\n",
    "alpha = 0.25\n",
    "clf = linear_model.Ridge(alpha = alpha)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# print(clf.coef_)\n",
    "# print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the coefficients just like we did with LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame({'col': X.columns, 'coef': clf.coef_})\n",
    "ax = sns.barplot(x = 'col', y = 'coef', data = dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some coefficients are close to zero. But we said eariler that Ridge regression cannot do feature selection like LASSO does. So what happened? What happened is that some coefficients being zero isn't necessarily due to regularization. It could be that those coefficients were just not useful in the first place. To really compare ridge and lasso, we have to see how the coefficients change when we change `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['alpha', 'col', 'coef'])\n",
    "\n",
    "for alpha in np.arange(0.01, 5, .01):\n",
    "    clf = linear_model.Ridge(alpha = alpha)\n",
    "    clf.fit(X, y)\n",
    "    dd = pd.DataFrame({'col': X.columns, 'coef': clf.coef_})\n",
    "    dd['alpha'] = alpha\n",
    "    results = results.append(dd, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the value of coefficients against `alpha` just as we did with LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x = 'alpha', y = 'coef', hue = 'col', data = results)\n",
    "ax.legend(loc = 'center right', bbox_to_anchor = (1.5, 0.5), ncol = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that unlike Lasso, Ridge regression's coefficients gradually (asymptotically) shrink as we increase the shrinkage constant `alpha`. So L2-regularization in ridge regression does not result in feature selection like it did with LASSO. So what is it good for? Even though L2-regularization doesn't necessarily reduce the number of features, it is still regularization, which means that it helps us fight against overfitting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
