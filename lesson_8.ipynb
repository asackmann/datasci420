{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic neural nework is called a **multi-layered perception**. It is a neural network where the neurons in each layer are connected to **all** the neurons in the next layer. For this reason we call it a **dense** network.\n",
    "\n",
    "In this notebook we use `numpy` to manually create and train a neural network. We do this mostly so we can build some intuition around what happens behind the scene when we train a neural network.\n",
    "\n",
    "The data we use is manually created. We want to have a very small data set so that we can look at intermediate results as we build our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X1 = np.array(np.arange(0.1, 0.7, 0.1))\n",
    "X1 = np.exp(X1 * 1.1 + 0.75)\n",
    "X2 = np.array(np.arange(0.6, 1.2, 0.1))\n",
    "X2 = np.exp(X2 * 0.4 + 0.75)\n",
    "X3 = np.random.random(6)\n",
    "X3 = np.exp(X3 * 0.4 + 0.75)\n",
    "\n",
    "X_train = np.array([X1, X2, X3]).T\n",
    "y_train = (X_train[:,:1] > 3).all(axis = 1).reshape(6, 1)\n",
    "\n",
    "print(np.hstack([X_train, y_train]))\n",
    "del X1, X2, X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a logistic regression\n",
    "\n",
    "Before we train a neural network, it might be worthwhile asking what we would do if we had to solve this using the tools we already have at our disposal. Since our target is binary, using a `LogisticRegression` is one easy option. So let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmod = LogisticRegression()\n",
    "logmod.fit(X_train, y_train.ravel())\n",
    "y_hat = logmod.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model. Usually we would evaluate the model on the training data. We'll worry about a test data later. For now that's besides the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with logistic regression, we can train a model that seems to quickly find the decision boundary. How does logistic regression make its prediction? It uses the following formula to get raw predictions .\n",
    "\n",
    "$$\\text{raw_predictions} = b_0 + b_1x_1 + b_2x_2 + b_3x_3$$\n",
    "\n",
    "In previous lectures, we referred to $b_0$, $b_1$ and $b_2$ as the model's **parameters**: $b_0$ is called the **intercept** and $b_1$, $b_2$ and $b_3$ are called **coefficients**. These raw predictions represent our confidence about how likely it is that any row of the data would belong to the positive class. But the scale of the these raw predictions are somewhat arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model intercept (bias): \")\n",
    "print(logmod.intercept_)\n",
    "print(\"Model coefficients (weights): \")\n",
    "print(logmod.coef_.T)\n",
    "\n",
    "pred = logmod.intercept_ + np.dot(X_train, logmod.coef_.T)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of an **activation function**. Here we use the **sigmoid** activation function, also called the **logistic** activation function, given by $\\sigma(z) = \\frac{1}{1+e^-z}$. It forces the activations to be between 0 and 1. Before passing the input to this function, use use `np.clip` to trim it between -500 and 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the raw predictions and pass them to a **sigmoid** function and get predictions that are rescaled to be between 0 and 1. We interpret the these scaled predictions as the probability that a given row belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.hstack([sigmoid(pred), y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the the above is is what we obtain when we run the `predict_proba` method of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmod.predict_proba(X_train) # the second column shows the probability of Y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when the prediction is below 0.50 the labels are 0 and otherwise the labels are 1. The reason we started with `LogisticRegression` is because the way that it trains is very similar to a neural network. In fact, a logistic regression model is a neural network with **no hidden layer**. So let's now manually create our neural network and see how we can get a result similar to what `LogisticRegression` obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Let's return to our prediction equation:\n",
    "\n",
    "$$\\text{raw_predictions} = b_0 + b_1x_1 + b_2x_2 + b_3x_3$$\n",
    "\n",
    "In neural networks, we prefer to use the word **bias** for the intercept and **weights** for the coefficients. We saw how logistic regression found its parameters. Now we want to see how a neural network find its parameters? It starts with some random values for them. We call this **random initialization**. We usally generate numbers that are random but close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(dim1, dim2 = 1, std = 1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1, dim2]) * std)\n",
    "    else:\n",
    "        return(np.zeros([dim1, dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have values for the parameters, we can now run a **forward pass**, which ultimately ends in **predictions**. Of course, because we randomly initialized our parameters, the first time around the predictions are as good as coin cosses.\n",
    "\n",
    "Note that our forward pass consists of a matrix multiplication, for which we use `np.dot`. The forward pass takes the input data and multiplies it by weights and adds the bias, the result of which is called a **weighted sum**, called `Z1` below. It then applies the **activation function** to the weighted sum, we get the **activations**, called `A1` here. \n",
    "\n",
    "In this example, we don't have any hidden layers, so our forward pass will take us directly from the data to the predictions. But if we had hidden layers, we would run this same calculation once for each hidden layers, finally finishing with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W1, bias, X):\n",
    "    Z1 = np.dot(X, W1) + bias\n",
    "    A1 = sigmoid(Z1)\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, input_cols = X_train.shape\n",
    "_, output_cols = y_train.shape\n",
    "\n",
    "weights = init_parameters(input_cols, output_cols)\n",
    "bias = init_parameters(output_cols)\n",
    "\n",
    "print(\"Checking dimensions: {} * {} + {}\".format(X_train.shape, weights.shape, bias.shape))\n",
    "\n",
    "pred = forward(weights, bias, X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the forward pass, we now have a prediction. Our next question is how can we imporve our prediction? The answer is that we need to calculate our error (called **loss**) and from that derive the **derivative of loss w.r.t. weights and biases**. In mulivariate calculus, this derivatite is called the **gradient**.\n",
    "\n",
    "In previous lectures, we learned that for classification model, we can measure the error by looking at **accuracy** (or precision and recall for imbalanced data). However, as it turns out these metrics are not going to work well here, because in order to get derivatives in calculus we need **continuous functions**, and accuracy, precision or recall are not continuous functions of our weights and biases. Another problem is that these metrics are obtained **after** we define our threshold, and can change if we change our threshold. So we need something else.\n",
    "\n",
    "One loss function that works well with classification is the **cross-entropy loss**. For binary classification, cross-entropy for the $i$th data point is defined as $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i)$, where $y_i$ is our binary target, and $\\hat y_i$ is the prediction (activation at the output layer) at row $i$. Cross-entropy for the whole data is just the average of the cross-entropies at each row.\n",
    "\n",
    "While we don't show the derivation here, once we define our loss function, we can get the derivative of loss w.r.t. the activations `A1`, and then (using the chain rule) get the derivative of loss w.r.t. to `Z1`, and finally w.r.t. weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(A1, W1, bias, X, Y):\n",
    "\n",
    "    m = np.shape(X)[0] # used the calculate the cost by the number of inputs -1/m\n",
    "   \n",
    "    loss = Y * np.log(A1) + (1 - Y)*np.log(1 - A1)           # loss at each row\n",
    "    cost = (-1/m) * np.sum(loss)                             # loss across all rows\n",
    "    dZ1 = A1 - Y                                             # derivative of loss wrt Z1\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)                           # derivative of loss wrt weights\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 0, keepdims = True) # derivative of loss wrt bias\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"dB1\": dBias}                       # updated weights and biases\n",
    "    \n",
    "    return(grads, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's once again test the output to make sure it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradients, _ = backward(pred, weights, bias, X_train, y_train)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all we need to start running our optimazion routine: a simple implementation of **gradient descent**. This cosists of iteratively running forward propagation to get predictions, the backpropagation to get the gradient of loss w.r.t. weights and biases, and finally moving weights and biases in the direction of their gradient. For the latter, we control the size of the step using a constant we call the **learning rate**. As we do this, we record loss at each iteration so that we can plot it and make sure that loss is decreasing at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_desc(num_epochs, learning_rate, X, Y):\n",
    "    \n",
    "    m, input_cols = X.shape\n",
    "    \n",
    "    W1 = init_parameters(input_cols, output_cols)\n",
    "    B1 = init_parameters(output_cols)\n",
    "    \n",
    "    loss_array = np.ones([num_epochs])*np.nan     # place-holder of keeping track of loss\n",
    "    \n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = forward(W1, B1, X)                   # get activations in final layer\n",
    "        grads, cost = backward(A1, W1, B1, X, Y)  # get gradient and the cost from BP \n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]      # update weights\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]      # update bias\n",
    "        \n",
    "        loss_array[i] = cost                      # record loss for current iteration\n",
    "        \n",
    "        parameter = {\"W1\": W1, \"B1\": B1}          # record parameters for current iteration\n",
    "    \n",
    "    return(parameter, loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Let's now run our gradient descent function for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "params, loss_array = run_grad_desc(num_epochs, learning_rate, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After letting the network train for many iterations, these are the final parameters we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params['B1'][0])\n",
    "print(params['W1'].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the parameters we got when we trained a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logmod.intercept_)\n",
    "print(logmod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the parameters don't necessarily look similar. This can be a lot of reasons for that. Because our data is close to linearly separable, there are a lot of possible solutions. There could also be differences between the `sklearn` logistic regression and our implementation of neural networks. So instead of comparing the parameters, let's compare the predictions: we can put the predictions we get from using the parameters for the neural network and logistic regression side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_nn = params['B1'] + np.dot(X_train, params['W1'])\n",
    "y_pred_logit = logmod.intercept_ + np.dot(X_train, logmod.coef_.T)\n",
    "\n",
    "np.hstack([y_pred_logit, Y_pred_nn, y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that these are raw predictions. So it might be best to pass these to a sigmoid function to turn them nito probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([sigmoid(y_pred_logit), sigmoid(Y_pred_nn), y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in either case if we use 0.50 as the cut-off both models predict correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Run the neural network for 10K iterations insead of 1000 and look at the predictions.\n",
    "- Run the neural network for 100K iterations insead of 1000 and look at the predictions.\n",
    "- Do you see a trend?\n",
    "- Return to where you run `run_grad_desc` and prior to running run the following code: `y_train = np.hstack([y_train, ~y_train])`. Careful! This will break the earlier logistic regression code. Train the network and look at the results that follow. Can you explain what happened? This result has important consequences for our earlier claim that you can do multi-class classification with neural networks using a single model (without resorting to **one-vs-rest** or **one-vs-one** models).\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show how loss drops iteration over iteration that we train this. This means that in a real-world scenario, if we let training continue indefinitely, eventually we will reach a point where we begin over-fitting to the training data. So it's important to have a test data set aside that we use for knowing when that happens so we can stop training. This is called **early stopping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = loss_array[::100]); # we only plot every 50th point so plot renders fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we saw how a neural network works. Of course a real neural networks would have at least one hidden layer, but hidden layers only add the amount of computation we have to do. The principle stays the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
